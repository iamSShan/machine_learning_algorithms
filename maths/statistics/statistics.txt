Data:
====
Data are of two types:
 - Qualitative: Qualitative data deals with characteristics	and descriptors that can't be easily measured but can be observed subjectively.
 - Quantitative: Quantitative data deals with numbers and things you can measure objectively.

 Qualitative can be divided into:
 - Nominal data: Data with no inherent order or ranking such as gender or race, such kind of data is called Nominal data.
 - Ordinal data: Data with ordered series. Like Ratings (Good, Avg, Bad) for different customers.

 Quantitative can be divided into:
 - Discrete data: Also known as categorical data, it can hold number of possible values. E.g: Number of students in class, email is spam or not.
 - Continuous data: Data that can hold infinite number of possible values. Eg: Weight of the person.


Statistics:
==========

 Statistics: Area of applied mathematics concerned with the data collection, analysis, interpretation and presentation.
 - Applied mathematics involves the application of mathematics to problems which arise in various areas, e.g. physics, engineering, medicine, biology, business, computer science, and industry, and/or the development of new or improved methods to meet the challenges of new problems. Thus, Applied mathematics is a combination of mathematical science and specialized knowledge.

- We view applied math as the application of mathematics to real-world problems with the dual goal of explaining observed phenomena and predicting new, as yet unobserved, phenomena. Therefore, the emphasis is on both the mathematics, e.g. the development of new methods to meet the challenges of new problems, and the real world.
 
 Basic terminologies used in Statistics:
 ======================================
 - Population: A collection or set of individuals or objects or events whose properties are to be analyzed.
 - Sample: A subset of population.
 - Population Mean: Mean of population. Denoted by μ.
 - Sample mean: Mean of a sample. Denoted by x̄.
 
Random variables and its types: 
===============================
- A random variable is a numerical description of the outcome of a statistical experiment. A random variable that may assume only a finite number or an infinite sequence of values is said to be discrete; one that may assume any value in some interval on the real number line is said to be continuous.
- For eg: x = 24; here x is a random variable storing 24 as its value, y = "yoo"; here y is also random variable.
Types of random variable:
- Numerical random variable: Like age
  Two types of numerical random variable:
  -- Discrete random variable: number of people in family(whole number and can't be negative)
  -- Continuous random variable: weight of people in family, salary(it can be whole number or also can be float)
- Categorical random variable: Like gender


Mean, Median and Mode:
=====================
A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data. As such, measures of central tendency are sometimes called measures of central location.

Sample of height = {168, 170, 150, 160, 182, 140, 175}
Here mean(x̄) = sum of heights/n = 63.5
Mean specifies measure of central tendency. Which means most of the height will be falling in this region.

For e.g: x = [1,2,3,4,5]
Here, mean: is 3 for above case
But let's say, if we add a outlier to the list x = [1,2,3,4,5,50]; then mean will become 10.8
So we get a big number and different output from previous, so in this case if we want to find out measure of central tendency, we can use median.
For median, we sort the numbers first, then we take the central number. If total numbers are even them take average of middle two numbers. Therefore here median will be (3+4)/2 = 3.5

Mode: Number with maximum frequency.
For e.g: x = [1,2,3,3,4,5]
Mean here: 3 // Here 3 is measure of central tendency.

Variance and Standard Deviation:
===============================
Sample of height = {168, 170, 150, 160, 182, 140, 175}
Variance = v = 1/n Σ(i=1 to n) (xi- μ)^2  // μ is mean
Standard deviation = √v

- Variance measures how far a set of data is spread out. A variance of zero indicates that all of the data values are identical. All non-zero variances are positive. A small variance indicates that the data points tend to be very close to the mean, and to each other. A high variance indicates that the data points are very spread out from the mean, and from one another. Variance is the average of the squared distances from each point to the mean.

- Standard Deviation is a measure of how spread out numbers are. Its symbol is σ (the greek letter sigma). Or you can say Standard deviation shows how much variation (dispersion, spread, scatter) from the mean exists. It represents a "typical" deviation from the mean. It is a popular measure of variability because it returns to the original units of measure of the data set. A low standard deviation indicates that the data points tend to be very close to the mean. A high standard deviation indicates that the data points are spread out over a large range of values.
The standard deviation can be thought of as a "standard" way of knowing what is normal (typical), what is very large, and what is very small in the data set.

Covariance:
==========
Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.
Formula is:
Cov(X,Y) = Σ (Xi-μ)*(Yi-ν) / n where:

X is a random variable
μ is the expected value (the mean) of the random variable X and
ν is the expected value (the mean) of the random variable Y
n = the number of items in the data set
Let's if X increases and Y also increases, covariance will be +ve, if X increases and Y decreases it will be -ve

But it doesn't tell how much positive or negative it will be, therefore we use another technique called Pearson Correlation Coefficient.

Pearson Correlation Coefficient:
===============================
Let's say if we have two features Height(independent feature) and Weight(dependent feature), then how we can find relationship between these two features. Relationship means If Height increases will Weight increase or not or if Height decreases, will Weight decrease or not. Now we can find it using covariance, but to also find it's magnitude we use Pearson Correlation Coefficient. 
Formula:
Pearson Correlation Coefficient(X, Y) = Cov(X,Y)/ (σx*σy) // Where σx is std dev of X and σy is std dev of Y
Here the value always ranges b/w -1 to 1
For e.g, if we have two independent features X and Y and a dependent variable Z, and if Pearson Correlation Coefficient is 1 then we can say both features are similar(as X increases then Y increases) then we can drop one of the feature and apply ML algorithm using one of the independent feature and the dependent feature.


Spearman's rank correlation coefficient:
=======================================
It is more better option than Pearson Correlation Coefficient.
The Spearman rank correlation coefficient, rs, is the nonparametric version of the Pearson correlation coefficient. Your data must be ordinal, interval or ratio. Spearman’s returns a value from -1 to 1, where:
+1 = a perfect positive correlation between ranks
-1 = a perfect negative correlation between ranks
0 = no correlation between ranks.
(When the word “non parametric” is used in stats, it doesn’t quite mean that you know nothing about the population. It usually means that you know the population data does not have a normal distribution.)

Formula:
Spearman's rank correlation coefficient(X, Y) = Cov(Xrank,Yrank)/ (σrx*σry) 
// Where we are calcuating covariance of Rank of X and Y
// Where σrx is std dev of rank of X and σy is std dev of rank of Y

Only if all n ranks are distinct integers, it can be computed using the popular formula
rs = 1 -  (6Σdi^2/(n(n^2)-1)
// where di = rg(Xi) - rg(Yi) is the difference between the two ranks of each observation,
// n is the number of observations

For e.g:
The raw data in the table below is used to calculate the correlation between the IQ of a person with the number of hours spent in front of TV per week

IQ  	Hours of TV per week
106	 	7
100		27
86		2
101		50
99		28
103		29
97		20
113		12
112		6
110		17

Firstly, evaluate di^2. To do so use the following steps, reflected in the table below.
	- Sort the data by the first column Xi. Create a new column xi and assign it the ranked values 1, 2, 3, ..., n.
	- Next, sort the data by the second column Yi. Create a fourth column yi and similarly assign it the ranked values 1, 2, 3, ..., n.
	- Create a fifth column di to hold the differences between the two rank columns xi and yi
	- Create one final column di^2 to hold the value of column di squared.

IQ(Xi)	Hours of TV per week(Yi)	rank xi	 rank yi	 di	    di^2
86		2							1		 1		     0		0
97		20							2		 6		     −4		16
99		28							3		 8		     −5		25
100		27							4		 7		     −3		9
101		50							5		 10		     −5		25
103		29							6		 9		     −3		9
106		7							7		 3		     4		16
110		17							8		 5		     3		9
112		6							9		 2		     7		49
113		12							10		 4		     6		36
With di^2 found, add them to find Σdi^2 = 194. The value of n is 10. These values can now be substituted back into the equation:
p = 1 -  (6Σdi^2/(n(n^2)-1)
We get,
p = 1 - (6*194/10(10^2 - 1))
p = −29/165 = −0.175757575
That the value is close to zero shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above


Gaussian Distribution(or normal Distribution):
=============================================
It is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve. 
In other words, if we have a random variable 'x' which belongs to gaussian distribution with some mean and standard deviation. If this 'x' variables follows the gaussian distribution, then if we take this random variable 'x' and we plot a histogram and probability density function, then we will get a bell curve.
In this curve, central point is mean, if we go one position to right it is μ+σ, for left μ-σ, if we go to two positions to left then it is μ-2σ and for two positions right it is μ+2σ and so on.

Empirical formula in Gaussian Dist:
	Prob(μ-σ <= x <= μ+σ) ~ 68%  // Range of 1st standard deviation; it means 68% of data points belonging to random variable x falls within range of 1st standard deviation.
	Prob(μ-2σ <= x <= μ+2σ) ~ 95%  // Range of 2nd standard deviation
	Prob(μ-3σ <= x <= μ+3σ) ~ 99.7%  // Range of 3rd standard deviation
	(Refer: https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2)
	The normal distribution is commonly associated with the 68-95-99.7 rule which you can see in the image above. 68% of the data is within 1 standard deviation (σ) of the mean (μ), 95% of the data is within 2 standard deviations (σ) of the mean (μ), and 99.7% of the data is within 3 standard deviations (σ) of the mean (μ).

Three points that should come to mind when talking about normal distribution:
- Bell shaped curve
- Area under the curve is 1
- Bell shaped curve is symmetrical


Standard normal form
====================
It is also a normal distribution but with some added properties
- μ(mean or population average) is 0
- σ(population std dev) is 1 

Chebyshev's Inequality:
======================
We know if a random variable X ∈ gaussian distribution(with some mean and std dev) then we have empirical formula telling that how much percentage of data points fall in the regions.(68-95-99.7)
Let's say, if we have a random variable Y, which ∉ gaussian distribution, then how we can find percentage of data points falling in the those ranges. We can use chebyshev's inequality.

It says that, if we have to find out the probability of a random variable falling within the range of standard deviation
i.e. Prob(μ-σ <= x < μ+σ) , it will be >= 1- 1/k^2
k means for which range of std deviation we have to find percentage of data points
Prob(μ-kσ <= x < μ+kσ) >= 1- 1/k^2

For k = 2 => Prob(μ-2σ <= x < μ+2σ) >= 1- 1/2^2 => Prob(μ-2σ <= x < μ+2σ) >= 3/4 => Prob(μ-2σ <= x < μ+2σ) >= 75%
For k = 3 => Prob(μ-3σ <= x < μ+3σ) >= 1- 1/3^2 => Prob(μ-3σ <= x < μ+3σ) >= 8/9 => Prob(μ-2σ <= x < μ+2σ) >= 88.8%


Log Normal Distribution:
=======================
Here X(random var.) ∈ log normal distribution; if ln(X) is normally distributed
For e.g: X = {x1, x2,...xn}
Logs = {log(x1), log(x2), log(x2)....log(xn)}; if we plot these values and if it normally distributed(we get a bell curve), then X ∈ log normal distribution.
In log normal distribution, curve is similar as bell curve but at the right side end it goes constant. We can see this curve in income.


Central limit theorem:
=====================
Let's consider a random variable `x`, which may or may not belong to gaussian distribution.
Then central limit theorem specifies:
(Refer: https://www.youtube.com/watch?v=PUBZC2MJ50Y&list=PLZoTAELRMXVMhVyr3Ri9IQ-t5QPBtxzJO&index=10)



PDF(Probability Density Function):
=================================
- In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample. 
- In other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there are an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would equal one sample compared to the other sample.
- Probability density function (PDF) is a statistical expression that defines a probability distribution (the likelihood of an outcome) for a discrete random variable (e.g., a stock or ETF) as opposed to a continuous random variable. The difference between a discrete random variable is that you can identify an exact value of the variable. For instance, the value for the variable, e.g., a stock price, only goes two decimal points beyond the decimal (e.g. 52.55), while a continuous variable could have an infinite number of values (e.g. 52.5572389658…).

Outlier:
=======
An outlier is a data point in a data set that is distant from all other observations. A data point that lies outside the overall distribution of the dataset.

Criteria to identify an outlier:
- Data point that falls outside of 1.5 times of an interquartile range above the 3rd quartile and below the 1st quartile.
- Data point that falls outside of 3 standard deviations. we can use a z score and if the z score falls outside of 2 standard deviation.

Reason for an outlier to exists in a dataset:
- Variability in the data
- An experimental measurement error

Impacts of having outliers in a dataset:
- It causes various problems during our statistical analysis.
- It may cause a significant impact on the mean and the standard deviation.

Various ways of finding the outlier:
- Using scatter plots (by plotting this we can visualize outliers)
- Box plot
- using z score
- using the IQR interquartile range

Z-score:
-------
- A Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean. If a Z-score is 0, it indicates that the data point's score is identical to the mean score. A Z-score of 1.0 would indicate a value that is one standard deviation from the mean. Z-scores may be positive or negative, with a positive value indicating the score is above the mean and a negative score indicating it is below the mean.
- A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is.
- A z-score can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the mean μ and also the population standard deviation σ. If any data point falls away from this 3rd standard deviation then it will considered as outlier.
- The basic z score formula for a sample is:   z = (x – μ) / σ
- Using this formula we are able to convert the data in Standard normal form(where mean = 0 and std dev(σ) = 1)
- In feature engineering we use use a normalization technique k/a standard normalization, where we apply this formula only for each feature.
- Now let's say if we want to find at 1.5 away from the mean, what will be std dev. We can't find normally using empirical formula. Therefore here Z-score and Z-score table concept comes.

IQR(inter quartile range):
-----------
If we have some numbers: 16, 5, 3, 7, 1, 2, 8, 10
First we sort them: 1, 2, 3, 5, 7, 8, 10, 16
Here 1 is 0th percentile(which means there are 0% of numbers that are less than 1)
Here 2 will 10th percentile(as we have 10 numbers total[0, 10, 20, 30, 40..100]); only 10% numbers are less than 2(10/100 *10 = 1 number)
Similarly 3 is 20th percentile and goes on.

In IQR, we focus on 25 percentile and 75 percentile, then 
IQR = 75%-25% 


Normalization vs Standarization in Feature Scaling:
==================================================
- Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization.
- It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.
- Example: If an algorithm is not using feature scaling method then it can consider the value 3000 meter to be greater than 5 km but that’s actually not true and in this case, the algorithm will give wrong predictions. So, we use Feature Scaling to bring all values to same magnitudes and thus, tackle this issue.

- Normalization helps us to scale down the feature between 0 and 1.
- Standarization will help to scale down the feature based on standard normal distribution. In standard normal distribution, mean is usually 0 and std dev is usually 1.


Sampling:
========
* Sampling helps a lot in research. It is one of the most important factors which determines the accuracy of your research/survey result.
* Population is the collection of the elements which has some or the other characteristic in common. Number of elements in the population is the size of the population.
* Sample is the subset of the population. The process of selecting a sample is known as sampling. Number of elements in the sample is the sample size.

* Different types of sampling techniques used in statistics:

- Probabilistic sampling(probability sampling):
  --------------------------------------------
  * This Sampling technique uses randomization to make sure that every element of the population gets an equal chance to be part of the selected sample. It’s alternatively known as random sampling.
  * Types of Probabilistic sampling techniques:

  i) Simple random sampling: In this case each individual is chosen entirely by chance and each member of the population has an equal chance, or probability. It is used when we don’t have any kind of prior information about the target population. 	For e.g: In cricket when we used to do numbering, one person randomly chooses number of batting position of every player while another person pointing out the number by hand backside of 1st person. This is called Simple Random Sampling
  
  ii) Systematic sampling:
   - Here the selection of elements is systematic and not random except the first element.
   - Individuals are selected at regular intervals from the sampling frame. The intervals are chosen to ensure an adequate sample size. If you need a sample size n from a population of size x, you should select every x/nth individual for the sample.
   - For example, if you wanted a sample size of 100 from a population of 1000, select every 1000/100 = 10th member of the sampling frame. 
   To get sample of size n, we divide our population of size N into subgroups of k elements.
   We select our first element randomly from the first subgroup of k elements. To select other elements of sample, perform following:
   	-- We know number of elements in each group is k i.e N/n
   	-- So if our first element is n1 then Second element is n1+k i.e n2, Third element n2+k i.e n3 and so on..
   	-- Taking an example of N=20, n=5; No of elements in each of the subgroups is N/n i.e 20/5 =4= k
   	-- Now, randomly select first element from the first subgroup. If we select n1= 3
   	-- n2 = n1+k = 3+4 = 7; n3 = n2+k = 7+4 = 11
  - This type of sampling is known as Systematic Random Sampling.

  iii) Stratified sampling: 
  In this method, the population is first divided into subgroups (or strata) who all share a similar characteristic. It is used when we might reasonably expect the measurement of interest to vary between the different subgroups, and we want to ensure representation from all the subgroups. For example, in a study of stroke outcomes, we may stratify the population by sex, to ensure equal representation of men and women. The study sample is then obtained by taking equal sample sizes from each stratum. In stratified sampling, it may also be appropriate to choose non-equal sample sizes from each stratum. Here we need to have prior information about the population to create subgroups.

  In school prayer in the morning there is particular line for every class and different line for boys and girls, so here there are n groups of particular class student. And here if teacher chooses one boy from every class or one girl from every class, this is known as Stratified Random Sampling
  
  iv) Cluster sampling: In a clustered sample, subgroups of the population are used as the sampling unit, rather than individuals. The population is divided into subgroups, known as clusters, then the clusters are randomly selected to be included in the study. Clusters are identified using details such as age, sex, location etc. Assume in school suddenly one guest come and principal ordered every teacher to order all the students to come at playground. This is an immediate order so teacher and students can't make proper lines according to class wise. Here there can be or can't be students of different class are stood in one line. So here every line is called as Cluster. Now teacher order one student from every line, this is called Cluster Random Sampling.

  v) Multi-Stage Sampling: It is the combination of one or more methods described above.

- Non probabilistic sampling(non-probability sampling):
  ----------------------------------------------------
  * It does not rely on randomization. This technique is more reliant on the researcher’s ability to select elements for a sample.
  * Outcome of sampling might be biased and makes difficult for all the elements of population to be part of the sample equally.
  * This type of sampling is also known as non-random sampling.
  * Types of Non probabilistic sampling techniques:
  i) Quota sampling
  ii) Referral/Snowball sampling
  iii) Convenience sampling
  iv) Purposive sampling
  v) Voluntary Sampling
 
 (Read: https://towardsdatascience.com/sampling-techniques-a4e34111d808)


Left skewed and Right Skewed Distribution:
=========================================
 
