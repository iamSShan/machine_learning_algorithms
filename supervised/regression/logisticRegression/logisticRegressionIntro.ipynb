{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c4910a",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "* It is a classification algorithm that is used to predict the probability of categorical dependent variable.\n",
    "\n",
    "### Prerequisite for logistic regression:\n",
    "* Logistic regression is usually applied to problems where two set of data points are linearly separable(can be divided with help of a straight line).\n",
    "![Linearly Separable](images/linearlySep.jpg)\n",
    "\n",
    "* The data should be linearly separable or almost linearly separable, which means we could separate the data with help in line in 2D and plane in 3D or hyperplane in higher dimensions.\n",
    "* We get bad results if we apply logistic regression on non-linear data.\n",
    "\n",
    "### Why Logistic Regression instead of Linear Regression for categorical case\n",
    "* We can draw a best fit line here also but in the case when there are some outliers are present in the dataset then best fit line will be bit far away from the points and we can not predict correctly as there will be high error rate. This is the problem with linear regression because every time we have to create best fit line with respect to data points.\n",
    "* Also we can get output greater than 1 and less than 0, but for binary classification we just have two classes. one class is between 0.5 and 1 and other one is between 0 and 0.5\n",
    "* For example watch [this](https://www.youtube.com/watch?v=L_xBe7MbPwk&t=3m47s)\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "* Logistic regression is named for the function used at the core of the method, the logistic function.\n",
    "* Logistic regression uses an equation as the representation, very much like linear regression. Input values (X) are combined linearly using weights or coefficient values to predict an output value (y). A key difference from linear regression is that the output value is a discrete rather than continuous.\n",
    "* Logistic regression model consists of two components: sigmoid function and features with weights:\n",
    "![Sigmoid Function](images/sigmoid.png)\n",
    " - e = Euler's number which is 2.71828.\n",
    "\n",
    "* The sigmoid function g(z) takes features and weights z as an input and returns a result between 0 and 1. The output of the sigmoid function is an actual prediction ŷ.\n",
    "* Or we can say that, Sigmoid function is simply trying to convert independent variable into a expression of probability(which ranges b/w between 0 and 1) with respect to the dependent variable\n",
    "* Where z = x1*w1 + ... + xn*wn\n",
    "* If we plot the sigmoid function we can see curve like this:\n",
    "\n",
    "![Sigmoid Function](images/sig_curve.png)\n",
    "\n",
    "* Read: https://jinglescode.github.io/2019/05/07/why-linear-regression-is-not-suitable-for-classification/\n",
    "* It is named as ‘Logistic Regression’, because it’s underlying technique is quite the same as Linear Regression. The term “Logistic” is taken from the Logit function that is used in this method of classification. Logistic regression also does the same thing as Linear Regression but with one addition. It pass the result through a special function called logistic/sigmoid function to produce the output y.\n",
    "* Watch [this](https://www.youtube.com/watch?v=uFfsSgQgerw) to know why we use sigmoid function.\n",
    "\n",
    "\n",
    "### Working:\n",
    "* Assume this is the data we have:\n",
    "![Sample data](images/linSep2.jpeg)\n",
    "\n",
    "* Here we have to create a best fit line which can be used to linear separate the data. So equation of line: y =mx +c; where m is the slope, c is y-intercept, x is the data point and y is the output.\n",
    "* Here the best fit line is not calculated with help of linear regression, there is some other way for that first we need to find exact coefficients `m` and `c`, from which we will be able to understand which is the best fit line.\n",
    "* Now some assumptions for all the crosses in the image(all positive points) we will take +1 and for all circles(all negative points) are dentoted as -1.\n",
    "* Also let's assume best fit line is passing through origin then we can say `c`(y-intercept) will be 0. So our updated equation is y = mx\n",
    "* Now let's say if we have to find the distance between any one data point and the plane, by the help of linear algebra it can be given as: (mx+c) / || M || ; If we consider M as unit vector then it becomes 1 and also `c` is 0, as discussed in above point, so we get `mx`. Therefore we can say that distance between a data point to plane is `mx`.\n",
    "* Now as there are many data points then we can do summation. Then eqn will be ∑(i=1 to n) mi.xi\n",
    "* Also important point to note: If a data point is above the plane(As cross in above picture) then if we calculate the distance it will become +ve and if data point is below plane then distance is negative. \n",
    "\n",
    "* Let consider some cases,\n",
    "\t- Case 1: Let's consider any one cross point and we know that this xi is positive so here(as discussed in fourth point) we can denote it as y=+1 and distance for all cross points to the plane will be positive too(as discussed in last point). Therefore mx>0 and if we multiply y and mx we get value greater than zero: y * mx > 0. So we can say that whenever we get a positive value our xi (selected point) is correctly classified.\n",
    "\n",
    "\t- Case 2: Let's consider any one circle point and we know that this xi is negative so here we can denote it as y=-1 and distance for all cross points to the plane will be negative. Therefore mx < 0 and we multiply y and mx we get value greater than zero: -y * -mx > 0. So we can say that whenever we get a positive value our xi (selected point) is correctly classified.\n",
    "\n",
    "\t- Case 3: Let's assume there is circle data point above the plane and we need to see whether it is properly classified or not. So here y=-1 as it is actually a negative point but the distance will be positive as it is above the plane. Now if we multiply y * -mx < 0. It is less than 0 so it means this point is incorrectly classified.\n",
    "\n",
    "\n",
    "* So our cost function will be max ∑(i=1 to n) yi * mi * xi; It means if we want to create a best fit line which linearly separates the data points then summation of all the point along with the distance should be maximum. As we saw in above cases that whenever it is greater than 0 it correctly classifies all the points but if value of the yi * mxi is less than 0 then it is not classifying points correctly.\n",
    "\n",
    "* So we will be updating `m` coefficient until we get maximum of coefficient. As we can't update xi and yi because these are already given.\n",
    "\n",
    "![Example 2](images/eg2.png)\n",
    "\n",
    "* Let's consider an example, let's say red points are -ve points and green point are positive points.\n",
    "* We can also see one outlier. Now to get the best fit line we have to find out the cost function as defined above\n",
    "* Let's assume distance between green point and best fit line is +1 and distance between red point and plane is -1. and distance between red outlier and plane is +500. Now if we add all th point  to get max value for cost function we get 5(y=1 * -1 for each) + 5(-distance * y=-1) - 500(distance * y=-1); We get -490, but we had to maximize cost function and it is negative. The plane we can see is logically best fit line if we ignore outlier but due to outlier this line is not able to give max cost function. So another best fit line will be chosen which would not be actually a best fit line This is impact of outlier here.\n",
    "\n",
    "\n",
    "* Now we gave to prevent this from happening so we have to update the function.\n",
    "* We just add a function to our equation: max ∑(i=1 to n) F(yi * mi * xi)  ; where is our sigmoid function\n",
    "* We pass this multiplication value to sigmoid function.\n",
    "\n",
    "\n",
    "### Sigmoid Function:\n",
    "\n",
    "![Sigmoid Function1](images/sigmoid.png)\n",
    "\n",
    "* where θT is the weight(or slope `m` as used above)\n",
    "* We multiply yi(which is either 1 or -1) with mxi and this is our `z` and we pass it to sigmoid function.\n",
    "* Here value lies between 0 to 1. We can also have value ranging between -1 to 1 but for that we have to use another function.\n",
    "* It removes the effect of outlier.\n",
    "* There are two common ways to approach the optimization of the Logistic Regression. One is through loss minimizing with the use of gradient descent and the other is with the use of Maximum Likelihood Estimation.\n",
    "\t1. Loss Minimization: Weights are important part of Logistic Regression and other Machine Learning algorithms and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function.\n",
    "\tThe loss function is defined as:\n",
    "\t![Loss function1](images/lf1.jpg)\n",
    "\t\n",
    "\twhere m is number of samples and y is target class\n",
    "\n",
    "\t- The goal is to minimize the loss by means of increasing or decreasing the weights, which is commonly called fitting. Which weights should be bigger and which should be smaller? This can be decided by a function called Gradient descent. The Gradient descent is just the derivative of the loss function with respect to its weights. Read [this](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#step-by-step).\n",
    "\n",
    "\t![Gradient Descent](images/grad_des.png)\n",
    "\n",
    "\t2. Maximum likelihood estimation: The goal here is to maximize the likelihood we can achieve this through Gradient ascent(not descent). Gradient ascent is the same as gradient descent, except its goal is to maximize a function rather than minimizing it.\n",
    "\n",
    "\t![Maximum Likelihood](images/max_lik.jpg)\n",
    "\n",
    "\t(z is defined above)\n",
    "\n",
    "\tNow, the gradient of the log likelihood is the derivative of the log likelihood function.\n",
    "\t![Gradient Ascent](images/grad_asc.png)\n",
    "\n",
    "\n",
    "### Applications\n",
    "* Fraud Detection\n",
    "* Spam email or not\n",
    "* Disease diagnosis\n",
    "* Emergency detection\n",
    "\n",
    "### Notes\n",
    "* Here dataset should be free of missing values\n",
    "\n",
    "### Hyperparameters:\n",
    "In scikit learn implementation there are around 15 parameters, let's check out some of them:\n",
    "\n",
    "1. penalty: For applying regularization, possible values of penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’\n",
    "* Specify the norm of the penalty:\n",
    "  * None: no penalty is added;\n",
    "  * 'l2': ridge regression regularization\n",
    "  * 'l1': lasso regression regularization\n",
    "  * 'elasticnet': both L1 and L2 penalty terms are added.\n",
    "\n",
    "2. tol: default=1e-4, Stopping criteria for gradient descent, if we increase its value then gradient descent will stop early\n",
    "\n",
    "3. C(float), default=1.0; Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "\n",
    "4. class_weight: dict or ‘balanced’, default=None\n",
    " * We use this when we have imbalanced data, then we assign weights\n",
    " * Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
    "\n",
    "5. solver: {‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "\n",
    "  * Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspect\n",
    "  * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n",
    "  \n",
    "6. max_iter: int, default=100\n",
    " * Maximum number of iterations taken for the algorithm to converge.\n",
    "\n",
    "7. multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’\n",
    "\n",
    "  * If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. \n",
    "\n",
    "8. n_jobs: (int), default=None\n",
    " * Number of CPU cores used when parallelizing over classes if multi_class=’ovr’”. This parameter is ignored when the solver is set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or not. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "\n",
    "9. l1_ratio: (float), default=None\n",
    "  * The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'. Setting l1_ratio=0 is equivalent to using penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldlenv] *",
   "language": "python",
   "name": "conda-env-mldlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
