{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2467ce",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "* In 90's we had strong algorithms like Random Forest, SVM and Gradient Boost, but they had issue, but they can overfit sometimes, like random forests generally handle overfitting better than individual decision trees, they can still overfit noisy datasets and problem of scalability was also seen as data increases, these algorithms might not perform very well and will be slow with huge amount of data.\n",
    "* This problem was solved by XGBoost in 2014.\n",
    "* It is one of the best machine algorithm out there in machine learning.\n",
    "* **Actually XGBoost is not an algorithm, it is a library that is build on already existing Gradient Boosting algorithm.** In a nutshell, it is combination of Gradient Boosting and some software engineering concept.\n",
    "\n",
    "### History of XGBoost:\n",
    "**Why was Gradient Boosting selected as base algorithm, some factors which make Gradient boosting a powerful algorithm**:\n",
    "* Flexibility: We can use any loss function here in classification and regression specifically.\n",
    "* Performance: In most of the problems it gives good performance.\n",
    "* Robust: If we apply regularization then results are very robust.\n",
    "* It also handles missing values internally easily.\n",
    "* In Kaggle people used Gradient Boosting to win competitions.\n",
    "\n",
    "Now just overall performance and working on large amount of data was the main concern,\n",
    "So the creator of XGBoost after created this algorithm and participated in Kaggle competition and won, so everybody started noticing the algorithm, and started using this algorithm in competitions and started winning.\n",
    "Then XGBoost was made open source so other developers could also contribute to its development.\n",
    "\n",
    "\n",
    "## XGBoost features:\n",
    "1. **Flexibility**:\n",
    "   * Cross platform: XGBoost Models can be built on one OS and run on another OS: Windows, Linux, Mac. Most of the ML algo are cross platform these days \n",
    "   * Multiple language support: XGBoost created wrappers for many programming languages even like Java, Scala, C, C++, etc\n",
    "   * Integration with other libraries and tools: numpy scikit-learn, pandas, spark, dask\n",
    "   * Support all kinds of ML problems: Like classification, regression, time series, ranking problems, anomaly detection, etc.\n",
    "   \n",
    "2. **Speed**: Many optimization were applied on XGBoost to improve traning speed such as:\n",
    "    * **Parallel processing**: Boosting is a sequential process where each model output goes to another model and so on. Whole process is not parallellized, but inside each model we use parallel processing to build a decision tree for a each single model.\n",
    "    * **Optimized data structures**: Mostly ML algo stores data row wise, XGBoost stores data column wise. Parallel processing is possible because of we are using column wise storage.\n",
    "    * **Cache awareness**: Use cache memory to get items which are used frequently\n",
    "    * **Out of core computing**: If we have RAM 8 GB and we have dataset 10 GB, then XGBoost breaks data into chunks let's say of 2 GB and trains it and then sequentially train all data chunks. For this we use hyperparameter `tree_method` as we set it `hist` it uses out of core computing\n",
    "    * **Distributed computing**: XGBoost provides distributing computing feature, where we can divide the dataset and give it to different nodes and get model trained and then aggregate the results. Benefit of distributed computing over Out of core computing is parallelism as out of core is happening on single machine while here we use different machines and do in parallel. So faster training is there.\n",
    "    * **GPU support**: Graphics Processing Unit is a processor with many cores, less powerful but more in numbers. XGBoost has support of GPU. For this we use hyperparameter `tree_method` as we set it `gpu_hist`.\n",
    "    \n",
    "3. **Performance**: What all was included to improve XGBoost performance:\n",
    "    * **Regularized learning objective**: In XGBoost's loss function by default there is a regularization term, so overfitting is not caused here.\n",
    "    * **Sparsity aware split finding**: XGBoost has some mechanism which understands sparsity in data and also handles it. XGBoost when does splitting based on value like f1>4.5 split data into two parts, one time it keeps missing value in first child and second time in second child with other values. Now calculate gain wrt to both and whichever gain is high the missing value is kept in that case node.\n",
    "    * **Handling missing values**: So we don't need to impute mising values beforehand, XBBoost does it itself as explained in above point.\n",
    "    * **Efficient Split Finding(Weighted Quantile Sketch + Approximate Tree Learning)**: We creates bins of numbers in each decision tree while splitting, based on data distribution, if data is dense somewhere so small bins, if numbers are far apart then large range bins are created.\n",
    "    * **Tree Pruning**: XGBoost gives many options for tree pruning like pre, post and also we can use a gamma parameter, based on which it is decided that a new branch should be formed or not\n",
    "    \n",
    "\n",
    "* There are few more library which improved gradient boosting, like XGBoost:\n",
    "    * LightGBM is also gives great performance just like XGBoost, but it is more light weight. LightGBM is targeted for faster training, low memory usage, better accuracy, support for parallel, distributed and GPU training and is capable of handling large-scale data.\n",
    "    * CatBoost is another library which has one main feature of categorical feature support.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldlenv] *",
   "language": "python",
   "name": "conda-env-mldlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
