{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# sklearn is just used to import a random dataset and to split the dataset into train and test\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': '/home/triloq/anaconda2/envs/mldl/lib/python3.6/site-packages/sklearn/datasets/data/breast_cancer.csv'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use breast cancer dataset\n",
    "dataset = datasets.load_breast_cancer()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc7828752b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = dataset.data, dataset.target\n",
    "# Analyze the data\n",
    "# print(X)\n",
    "# print(X[:, 0])\n",
    "plt.scatter(X[:, 0], y, c='r', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# Size of X is m *n vector and y is 1d vector of size m(m is number of samples and n is number of features)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Using a sklearn function to split dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent update rules:\n",
    "![Update Rules](images/update_rules.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using gradient descent here:\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.0001, n_iters=1000):\n",
    "        self.learning_rate = learning_rate  # Learning rate should be less as we need to hit minimum\n",
    "        self.n_iters = n_iters  # no. of iterations we want\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # z = np.dot(X, weight) // commented coz we are already passing z value from fit or predict function(which also contains bias)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Initialize required parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent method:\n",
    "        for _ in range(self.n_iters):\n",
    "            # First we calculate linear model values using y = mx+b\n",
    "            linear_y = np.dot(X_train, self.weights) + self.bias\n",
    "            # Now we have to apply sigmoid function\n",
    "            y_pred = self.sigmoid(linear_y)\n",
    "            # Now we need to update our weights and bias, we can use formula as shown above\n",
    "            # This is same formula as mentioned in README file,, showing derivative of weight,\n",
    "            # also we can ignore 2 as it is just a scaling entity\n",
    "            dw = (1/n_samples) * np.dot(X_train.T, (y_pred-y_train))\n",
    "            db = (1/n_samples) * np.sum(y_pred-y_train)\n",
    "            self.weights -= self.learning_rate*dw \n",
    "            self.bias -= self.learning_rate*db \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # We first here approx our data wrt linear model\n",
    "        linear_y = np.dot(X_test, self.weights) + self.bias\n",
    "        # Then we apply sigmoid function\n",
    "        y_pred = self.sigmoid(linear_y)\n",
    "        # Now we want to tell whether it is class 0 or class 1, \n",
    "        # so we say if it predicted value is larger than 0.5 then it is 1 else if it less then we take it as 0\n",
    "        # Threshold here is considered as 0.5\n",
    "        y_pred_out = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "        return y_pred_out\n",
    "\n",
    "\n",
    "    def get_accuracy(self, y_test, y_pred):\n",
    "        # To get accuracy\n",
    "        accuracy = np.sum(y_test == y_pred) / len(y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "# Create object for LogisticRegression class\n",
    "logReg = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n",
    "# Train using training data\n",
    "logReg.fit(X_train, y_train)\n",
    "y_predicted = logReg.predict(X_test)\n",
    "\n",
    "accuracy = logReg.get_accuracy(y_test, y_predicted)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "# We are getting accracy of ~ 91.2%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldl] *",
   "language": "python",
   "name": "conda-env-mldl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
