{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b56cdd6",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "* The curse of dimensionality refers to various challenges that arise when dealing with high-dimensional data. As the number of dimensions or features increases, the amount of data required to cover the feature space adequately increases exponentially. This phenomenon can lead to several issues.\n",
    "* So for a model there is threshold let's say 5, if we increase features more than that there won't be any effect on model performance,  infact it may harm as it might increase runtime.\n",
    "* Curse of dimensionality can lead to several issues:\n",
    "\n",
    "    * **Increased Sparsity**: In high-dimensional spaces, data points become increasingly sparse, **meaning that the available data points are spread out across the space**, making it difficult to draw meaningful conclusions or build accurate models. For eg in real world let's say if we need to find a school bag, if we are given that is in the playground, or we are told it is inside whole school, or we are told it is withing entire society, so here finding school bag in first case is most feasible and more easily achievable.\n",
    "    * **Increased Computational Complexity**: Many algorithms suffer from computational inefficiency as the dimensionality of the data increases. This is because operations such as distance calculations, which are fundamental to many algorithms, become more computationally expensive in high-dimensional spaces.\n",
    "    * **Increased Overfitting**: With a high number of dimensions, there is a risk of overfitting, where a model learns noise or irrelevant patterns in the data, rather than capturing the underlying structure. High-dimensional data provides more opportunities for the model to fit to noise, leading to poorer generalization performance.\n",
    "    \n",
    "Overall, the curse of dimensionality highlights the need for careful feature selection, dimensionality reduction techniques, and thoughtful data preprocessing to mitigate the challenges associated with high-dimensional data.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldlenv] *",
   "language": "python",
   "name": "conda-env-mldlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
