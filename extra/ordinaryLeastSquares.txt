* Here in this method we want to find the line that minimizes the distances between actual and predicted, basically the sum of all distances
* We are sqaurring as we can get negative values after taking difference
* As we know eqn of line is: y = mx+b
* We will use Ordinary Least Squares method to find the best line intercept (b) slope (m)
* For, m = Σ(i to n) (xᵢ - x̄)(yᵢ - ȳ)/ Σ(i to n)(xᵢ - x̄)²
* For, b = ȳ - mx̄

x̄ = average value of independent variable
ȳ = average value of dependent variable

* While in gradient descent: The goal is similar like the above operation that we did to find out a best fit of intercept line ‘y’ in the slope ‘m’. Using Gradient descent algorithm also, we will figure out a minimal cost function by applying various parameters for theta 0 and theta 1 and see the slope intercept until it reaches convergence.