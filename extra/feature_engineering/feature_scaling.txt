Feature scaling:
- Feature Scaling is a technique to standardize the independent `numerical` features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes(or values) or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values. 
- This is the last step in data preprocessing and before model training.
- It is also called as data normalization.
- We fit feature scaling with train data and transform on testing data.

Every feature has two important components:
	- magnitude(value of the feature)
	- unit(method of measuring like kg, gm, cm, etc.)

Why to perform feature scaling:
- ML algorithm can't understand features units, can only understand magnitudes.

- Suppose there are two features called weights(kgs) and heights(in cms) and if we want to plot them, here if we don't perform feature scaling and we are using K-nearest neighbor, then the distance b/w each point will be big. Therefore we scale down the feature values. If not scaled, the feature with a higher value range starts dominating when calculating distances.

- Another example: A weight of 10 grams and a price of 10 dollars represents completely two different things — which is a no brainer for humans, but for a model as a feature, it treats both as same.

- Feature scaling refers to putting the values in the same range or same scale so that no variable is dominated by the other. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by scaling.

- If we are plotting a 2D graph, using these height and weights, then we will get varying distances which may be huge, when actually they are not very different.

- One more example can be: Let's say we have a DF containing heights of two person, column 1 contains height of first person in cms and column 2 contains height of second person in ft. So a row containg person1 height as 150 cm and person 2 height as 7 ft. We know that 7 ft is > than 150 cm, but here ML algorithm will consider 150 > 7. That's why we need to scale these features first.


When to perform feature scaling:
- Many algo works on concept of euclidean distance, manhattan distance, etc. Like K-nearest neighbor, K-means clustering, principal component ananlysis, gradient descent, etc.
- In Linear Regression, we can also use feature scaling as feature scaling works uses gradient descent in which we have to reach global minima, so if features are scaled to less value then global minima can be reached fast.
- In these type of algo magnitude plays an important role, as we can see much variety in each distances.
- Therefore in these cases where different features are in different units and these algorithms are used, we have to scale down the features value.


How to scale:
1) Standardisation(Z-score normalization): 
  * It rescales the features so that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.
  * Formula used here:
        x' = (x-μ)/σ  ; σ is standard deviation, μ is mean

  * It is sensitive to outliers since it uses the mean and standard deviation.
  * Suitable for algorithms that assume the data is normally distributed (e.g., linear regression, logistic regression).
  * Often used in Principal Component Analysis (PCA) and algorithms that calculate distances (e.g., K-nearest neighbors, SVM).
  * Sklearn StandardScaler library is used for this.
  * Standardization does not bound values between two specific numbers. Instead, it transforms the values such that they are centered around a mean of 0 

2) Normalization:
  * Normalization (also called Min-Max scaling) rescales the features to a fixed range, usually [0, 1] or [-1, 1].
  * Formula is : x' = ( x-min(x) ) / ( max(x)-min(x) )
  * It is sensitive to outliers since the min and max values can be affected by extreme values.
  * When the algorithm does not make assumptions about the distribution of the data we can use normalization.
  * Suitable for algorithms that rely on distance or magnitude, such as K-nearest neighbors (KNN), neural networks, and clustering algorithms like K-means.
  * When features have different units and need to be brought to the same scale.
  * We use MinMaxScaler in sklearn


Choosing Between Standardization and Normalization
*  Both techniques are sensitive to outliers, but normalization is generally more affected since it depends on the min and max values which can be skewed by extreme values.
* Standardization is often preferred when the features have different units and scales, and when the data is approximately normally distributed. It ensures that all features contribute equally to the model.
* Normalization is typically used when the scale of the features matters, and you need to bound the values, especially in distance-based algorithms or neural networks where the input range can affect the learning process.

3) Robust Normalization:
  * Robust normalization scales the data using the median and the interquartile range (IQR), making it robust to outliers.
  Formula: x′= (x − median(x)) / IQR
  * To use: from sklearn.preprocessing import RobustScaler

4) Mean Normalization:
  * Mean normalization scales the data to have a mean of 0 and ranges between -1 and 1.
  * Formula:   x′ = (x- mean(x)) / (max(x) - min(x))


Note: Naive Bayes, Linear Discriminant Analysis, tree based models, XGBoost are not affected by feature scaling. In short any algo which is not distance based is not affected by feature scaling. We can do here also, but mostly it doesn't affects.
Like in Decision Tree even if we apply feature scaling, number of branches will be almost same so it won't affect.
